{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Anil Kumar Rai\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\Dr. Anil Kumar Rai\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data in list1,list2 and label lists\n",
    "obj=None\n",
    "list1 = []\n",
    "seqlen1 = []\n",
    "list2 = []\n",
    "seqlen2 = []\n",
    "label = []\n",
    "with open(\"C:\\\\Users\\\\Dr. Anil Kumar Rai\\\\Desktop\\\\snli_1.0\\\\snli_1.0_train.jsonl\",\"r\") as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        sent1=obj['sentence1'].split()\n",
    "        sent2=obj['sentence2'].split()\n",
    "        seqlen1.append(len(sent1))\n",
    "        seqlen2.append(len(sent2))\n",
    "        lab=obj['gold_label']\n",
    "        list1.append(sent1)\n",
    "        list2.append(sent2)\n",
    "        label.append(lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train word2vec model\n",
    "w2vlist=list1+list2\n",
    "model = Word2Vec(w2vlist, size=200, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63037\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary for word2vec\n",
    "vocab={}\n",
    "ind=0\n",
    "for line in w2vlist:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word]=ind\n",
    "            ind=ind+1\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Anil Kumar Rai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "emb_size=200\n",
    "embeddings = np.zeros((vocab_size, emb_size),dtype=np.float32)\n",
    "for k, v in vocab.items():\n",
    "  embeddings[v] = model[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put premises in text_train and hypothesis in hypo_train\n",
    "text_train=[]\n",
    "hypo_train=[]\n",
    "\n",
    "train_lm=[]\n",
    "test_lm=[]\n",
    "for line in list1:\n",
    "    new_lis=[]\n",
    "    for word in line:\n",
    "        new_lis.append(vocab[word])\n",
    "        train_lm.append(vocab[word])\n",
    "    text_train.append(new_lis)\n",
    "\n",
    "for line in list2:\n",
    "    new_lis=[]\n",
    "    for word in line:\n",
    "        new_lis.append(vocab[word])\n",
    "        test_lm.append(vocab[word])\n",
    "    hypo_train.append(new_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lm[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data path to store session\n",
    "data_path=\"C\\\\Users\\\\Dr. Anil Kumar Rai\\\\Desktop\\\\lm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce a batch of data\n",
    "def batch_producer(raw_data, batch_size, num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len],[batch_size, batch_len])\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps]\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to take input\n",
    "class Input(object):\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "#Creates bidirectional LSTM model\n",
    "class Model(object):\n",
    "    def assign_lr(self, session, lr_value):\n",
    "            session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            inputs = tf.nn.embedding_lookup(embeddings, self.input_obj.input_data)\n",
    "        \n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "        self.init_state_fw = tf.placeholder(tf.float32, [2, self.batch_size, self.hidden_size])\n",
    "        self.init_state_bw = tf.placeholder(tf.float32, [2, self.batch_size, self.hidden_size])\n",
    "        n_init_state_fw = tf.unstack(self.init_state_fw, axis=0)\n",
    "        n_init_state_bw = tf.unstack(self.init_state_bw, axis=0)\n",
    "        rnn_tuple_state_fw = tf.contrib.rnn.LSTMStateTuple(n_init_state_fw[0],n_init_state_fw[1])\n",
    "        rnn_tuple_state_bw = tf.contrib.rnn.LSTMStateTuple(n_init_state_bw[0],n_init_state_bw[1])\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size, state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size, state_is_tuple=True)\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=dropout)\n",
    "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=dropout)\n",
    "        ((encoder_outputs_fw,encoder_outputs_bw), (self.state_fw,self.state_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, initial_state_fw=rnn_tuple_state_fw, initial_state_bw=rnn_tuple_state_bw, dtype=tf.float32)\n",
    "        encoder_outputs = tf.add(encoder_outputs_fw,encoder_outputs_bw) \n",
    "        output = tf.reshape(encoder_outputs, [-1, hidden_size])\n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits,\n",
    "                    self.input_obj.targets,\n",
    "                    tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "                    average_across_timesteps=False,\n",
    "                    average_across_batch=True)\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if not is_training:\n",
    "           return\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train the BiLSTM Model\n",
    "def train1(train_data, vocabulary, num_epochs, batch_size, model_save_name,\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n",
    "    # setup data and models\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "    m = Model(training_input, is_training=True, hidden_size=350, vocab_size=vocabulary)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            current_state_fw = np.zeros((2, batch_size, m.hidden_size))\n",
    "            current_state_bw = np.zeros((2, batch_size, m.hidden_size))\n",
    "            curr_time = dt.datetime.now()\n",
    "            for step in range(training_input.epoch_size):\n",
    "                if step % print_iter != 0:\n",
    "                    cost, _, current_state_fw, current_state_bw = sess.run([m.cost, m.train_op, m.state_fw, m.state_bw],\n",
    "                                                      feed_dict={m.init_state_fw: current_state_fw, m.init_state_bw: current_state_bw})\n",
    "                else:\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n",
    "                    curr_time = dt.datetime.now()\n",
    "                    cost, _, current_state_fw, current_state_bw, acc = sess.run([m.cost, m.train_op, m.state_fw, m.state_bw, m.accuracy],\n",
    "                                                           feed_dict={m.init_state_fw: current_state_fw, m.init_state_bw: current_state_bw})\n",
    "                    \n",
    "                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\".format(epoch, step, cost, acc, seconds))\n",
    "\n",
    "            # save a model checkpoint\n",
    "            saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "        # do a final save\n",
    "        saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'encoder_outputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5788f6d9a587>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_lm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-fddab8ac19cd>\u001b[0m in \u001b[0;36mtrain1\u001b[1;34m(train_data, vocabulary, num_epochs, batch_size, model_save_name, learning_rate, max_lr_epoch, lr_decay, print_iter)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# setup data and models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtraining_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0minit_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0morig_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d654258862c0>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input, is_training, hidden_size, vocab_size, dropout, init_scale)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mcell_bw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropoutWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_bw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_keep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mattention_mechanism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBahdanauAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_sequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mcell_fw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttentionWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_fw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mechanism\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_layer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcell_bw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttentionWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_bw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mechanism\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_layer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'encoder_outputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train1(train_lm,vocab_size,55,40,\"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path, test_data, reversed_dictionary):\n",
    "    test_input = Input(batch_size=20, num_steps=35, data=test_data)\n",
    "    m = Model(test_input, is_training=False, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=2)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))\n",
    "        # restore the trained model\n",
    "        saver.restore(sess, model_path)\n",
    "        # get an average accuracy over num_acc_batches\n",
    "        num_acc_batches = 30\n",
    "        check_batch_idx = 25\n",
    "        acc_check_thresh = 5\n",
    "        accuracy = 0\n",
    "        for batch in range(num_acc_batches):\n",
    "            if batch == check_batch_idx:\n",
    "                true_vals, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy],\n",
    "                                                               feed_dict={m.init_state: current_state})\n",
    "                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps]]\n",
    "                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]]\n",
    "                print(\"True values (1st line) vs predicted values (2nd line):\")\n",
    "                print(\" \".join(true_vals_string))\n",
    "                print(\" \".join(pred_string))\n",
    "            else:\n",
    "                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state: current_state})\n",
    "            if batch >= acc_check_thresh:\n",
    "                accuracy += acc\n",
    "        print(\"Average accuracy: {:.3f}\".format(accuracy / (num_acc_batches-acc_check_thresh)))\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if args.data_path:\n",
    "    data_path = args.data_path\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "if args.run_opt == 1:\n",
    "    train(train_data, vocabulary, num_layers=2, num_epochs=60, batch_size=20,\n",
    "          model_save_name='two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr')\n",
    "else:\n",
    "    trained_model = args.data_path + \"\\\\two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr-38\"\n",
    "    test(trained_model, test_data, reversed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Users\\DR8AF8~1.ANI\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/aggregation/scaling:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with aggregation/scaling\n",
      "INFO:tensorflow:Initialize variable module/aggregation/weights:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with aggregation/weights\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_0:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_0\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_1:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_1\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_2:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_2\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_3:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_3\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_4:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_4\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_5:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_5\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_6:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/W_cnn_6\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_0:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_0\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_1:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_1\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_2:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_2\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_3:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_3\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_4:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_4\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_5:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_5\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_6:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN/b_cnn_6\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/W_carry:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_0/W_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/W_transform:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_0/W_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/b_carry:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_0/b_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/b_transform:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_0/b_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/W_carry:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_1/W_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/W_transform:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_1/W_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/b_carry:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_1/b_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/b_transform:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_high_1/b_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_proj/W_proj:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_proj/W_proj\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_proj/b_proj:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/CNN_proj/b_proj\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/char_embed:0 from checkpoint b'C:\\\\Users\\\\DR8AF8~1.ANI\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d\\\\variables\\\\variables' with bilm/char_embed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'tokens': Expected string, got 0 of type 'int' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\tensor_info.py\u001b[0m in \u001b[0;36m_convert_to_compatible_tensor\u001b[1;34m(value, target, error_prefix)\u001b[0m\n\u001b[0;32m    116\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_or_indexed_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_or_indexed_slices\u001b[1;34m(value, dtype, name)\u001b[0m\n\u001b[0;32m   1229\u001b[0m   return internal_convert_to_tensor_or_indexed_slices(\n\u001b[1;32m-> 1230\u001b[1;33m       value=value, dtype=dtype, name=name, as_ref=False)\n\u001b[0m\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor_or_indexed_slices\u001b[1;34m(value, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     return internal_convert_to_tensor(\n\u001b[1;32m-> 1268\u001b[1;33m         value, dtype=dtype, name=name, as_ref=as_ref)\n\u001b[0m\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    216\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    195\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 196\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    197\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m       \u001b[0m_AssertCompatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    346\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[1;32m--> 347\u001b[1;33m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected string, got 0 of type 'int' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-054277caa9b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     },\n\u001b[0;32m     20\u001b[0m     \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tokens\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     as_dict=True)[\"elmo\"]\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, _sentinel, signature, as_dict)\u001b[0m\n\u001b[0;32m    196\u001b[0m     dict_inputs = _prepare_dict_inputs(\n\u001b[0;32m    197\u001b[0m         inputs, self._spec.get_input_info_dict(signature=signature,\n\u001b[1;32m--> 198\u001b[1;33m                                                tags=self._tags))\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     dict_outputs = self._impl.create_apply_graph(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m_prepare_dict_inputs\u001b[1;34m(inputs, tensor_info_map)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Signature expects multiple inputs. Use a dict.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[1;31m# Finally convert a dict of values into a dict of tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_compatible_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_info_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\tensor_info.py\u001b[0m in \u001b[0;36mmake_compatible_dict\u001b[1;34m(values, targets)\u001b[0m\n\u001b[0;32m    170\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     result[key] = _convert_to_compatible_tensor(\n\u001b[1;32m--> 172\u001b[1;33m         value, targets[key], error_prefix=\"Can't convert %r\" % key)\n\u001b[0m\u001b[0;32m    173\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\tensor_info.py\u001b[0m in \u001b[0;36m_convert_to_compatible_tensor\u001b[1;34m(value, target, error_prefix)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_or_indexed_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merror_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_is_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_is_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert 'tokens': Expected string, got 0 of type 'int' instead."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
